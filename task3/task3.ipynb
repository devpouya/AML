{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(5117, 17813)\n(3411, 17813)\n(5117,)\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_path = \"X_train.csv\"\n",
    "label_path = \"y_train.csv\"\n",
    "test_path = \"X_test.csv\"\n",
    "samples_path = \"samples.csv\"\n",
    "\n",
    "df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "labeldf = pd.read_csv(label_path)\n",
    "\n",
    "\n",
    "labels = labeldf.values\n",
    "data = df.values\n",
    "test_data = test_df.values\n",
    "\n",
    "data = data[:,1:]\n",
    "test_data = test_data[:,1:]\n",
    "labels = labels[:,1]\n",
    "\n",
    "\n",
    "print(data.shape)\n",
    "print(test_data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Class 0 Counts: 3030\nClass 1 Counts: 443\nClass 2 Counts: 1474\nClass 3 Counts: 170\n"
    }
   ],
   "source": [
    "def count_instances(labels):\n",
    "    unique, counts = np.unique(labels,return_counts=True)\n",
    "    return dict(zip(unique,counts))\n",
    "\n",
    "class_counts = count_instances(labels)\n",
    "\n",
    "print(\"Class 0 Counts: {}\".format(class_counts[0]))\n",
    "print(\"Class 1 Counts: {}\".format(class_counts[1]))\n",
    "print(\"Class 2 Counts: {}\".format(class_counts[2]))\n",
    "print(\"Class 3 Counts: {}\".format(class_counts[3]))\n",
    "\n",
    "\n",
    "(num_train, max_timesteps) = data.shape\n",
    "(num_test, _) = test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # truncate and pad input sequences\n",
    "max_review_length = data.shape[1]\n",
    "# X_train = sequence.pad_sequences(data, maxlen=max_review_length, padding='post')\n",
    "# X_test = sequence.pad_sequences(test_data, maxlen=max_review_length, padding='post')\n",
    "X_train = np.nan_to_num(data, nan = 0)\n",
    "X_test = np.nan_to_num(test_data,nan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(5117, 1, 17813)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "labels = keras.utils.to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "y_train = labeldf.values[:,1]\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "        \n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 4093 samples, validate on 1024 samples\nEpoch 1/250\n - 8s - loss: 1.2664 - accuracy: 0.5121 - val_loss: 1.1546 - val_accuracy: 0.5762\nEpoch 2/250\n - 7s - loss: 1.0583 - accuracy: 0.5959 - val_loss: 1.0308 - val_accuracy: 0.5801\nEpoch 3/250\n - 7s - loss: 0.9982 - accuracy: 0.5947 - val_loss: 1.0153 - val_accuracy: 0.5801\nEpoch 4/250\n - 7s - loss: 0.9839 - accuracy: 0.5952 - val_loss: 1.0278 - val_accuracy: 0.5781\nEpoch 5/250\n - 7s - loss: 0.9810 - accuracy: 0.5947 - val_loss: 1.0221 - val_accuracy: 0.5771\nEpoch 6/250\n - 7s - loss: 0.9752 - accuracy: 0.5947 - val_loss: 1.0245 - val_accuracy: 0.5791\nEpoch 7/250\n - 7s - loss: 0.9723 - accuracy: 0.5959 - val_loss: 1.0205 - val_accuracy: 0.5781\nEpoch 8/250\n - 7s - loss: 0.9692 - accuracy: 0.5976 - val_loss: 1.0193 - val_accuracy: 0.5771\nEpoch 9/250\n - 7s - loss: 0.9678 - accuracy: 0.5966 - val_loss: 1.0164 - val_accuracy: 0.5811\nEpoch 10/250\n - 7s - loss: 0.9666 - accuracy: 0.5971 - val_loss: 1.0111 - val_accuracy: 0.5830\nEpoch 11/250\n - 7s - loss: 0.9604 - accuracy: 0.6010 - val_loss: 1.0173 - val_accuracy: 0.5791\nEpoch 12/250\n - 7s - loss: 0.9570 - accuracy: 0.5998 - val_loss: 1.0236 - val_accuracy: 0.5771\nEpoch 13/250\n - 7s - loss: 0.9512 - accuracy: 0.6040 - val_loss: 1.0196 - val_accuracy: 0.5742\nEpoch 14/250\n - 7s - loss: 0.9470 - accuracy: 0.6042 - val_loss: 1.0259 - val_accuracy: 0.5762\nEpoch 15/250\n - 7s - loss: 0.9427 - accuracy: 0.6027 - val_loss: 1.0260 - val_accuracy: 0.5645\nEpoch 16/250\n - 7s - loss: 0.9455 - accuracy: 0.6057 - val_loss: 1.0329 - val_accuracy: 0.5713\nEpoch 17/250\n - 7s - loss: 0.9455 - accuracy: 0.6035 - val_loss: 1.0288 - val_accuracy: 0.5742\nEpoch 18/250\n - 7s - loss: 0.9410 - accuracy: 0.6025 - val_loss: 1.0223 - val_accuracy: 0.5732\nEpoch 19/250\n - 7s - loss: 0.9404 - accuracy: 0.6071 - val_loss: 1.0307 - val_accuracy: 0.5742\nEpoch 20/250\n - 7s - loss: 0.9360 - accuracy: 0.6150 - val_loss: 1.0344 - val_accuracy: 0.5654\nEpoch 21/250\n - 7s - loss: 0.9312 - accuracy: 0.6093 - val_loss: 1.0443 - val_accuracy: 0.5615\nEpoch 22/250\n - 7s - loss: 0.9322 - accuracy: 0.6108 - val_loss: 1.0427 - val_accuracy: 0.5576\nEpoch 23/250\n - 7s - loss: 0.9251 - accuracy: 0.6150 - val_loss: 1.0442 - val_accuracy: 0.5498\nEpoch 24/250\n - 7s - loss: 0.9221 - accuracy: 0.6125 - val_loss: 1.0507 - val_accuracy: 0.5576\nEpoch 25/250\n - 7s - loss: 0.9233 - accuracy: 0.6157 - val_loss: 1.0467 - val_accuracy: 0.5508\nEpoch 26/250\n - 7s - loss: 0.9208 - accuracy: 0.6167 - val_loss: 1.0464 - val_accuracy: 0.5410\nEpoch 27/250\n - 7s - loss: 0.9168 - accuracy: 0.6164 - val_loss: 1.0484 - val_accuracy: 0.5537\nEpoch 28/250\n - 7s - loss: 0.9128 - accuracy: 0.6196 - val_loss: 1.0533 - val_accuracy: 0.5488\nEpoch 29/250\n - 7s - loss: 0.9071 - accuracy: 0.6240 - val_loss: 1.0588 - val_accuracy: 0.5400\nEpoch 30/250\n - 7s - loss: 0.9054 - accuracy: 0.6220 - val_loss: 1.0625 - val_accuracy: 0.5371\nEpoch 00030: early stopping\n"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x7f7fe65c1f60>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "number_of_classes = 4\n",
    "batch_size = 32\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, return_sequences=True, input_shape=(1, 17813)))\n",
    "model.add(Dropout(0.25))\n",
    "# model.add(LSTM(256, return_sequences=True))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(LSTM(128, return_sequences=True))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(LSTM(64, return_sequences=True))\n",
    "# model.add(Dropout(0.25))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(number_of_classes, activation='softmax'))\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', f1_m])\n",
    "model.fit(X_train, labels, epochs=250, batch_size=batch_size, validation_split=0.2, verbose=2, shuffle=False, class_weight=class_weights, callbacks=[early_stopping])\n",
    "# model.save('Keras_models/my_model_' + str(i) + '_' + str(j) + '_' + str() + '.h5')\n",
    "# predictions = model.predict(X_val)\n",
    "# score = accuracy_score(change(Y_val), change(predictions))\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}